# ðŸ“š Books Scraping Pipeline

## ðŸš€ Contexte
Projet acadÃ©mique : collecter des donnÃ©es du site [books.toscrape.com](https://books.toscrape.com) puis les rendre accessibles via une API.

### Ã‰tapes :
1. **Scraping (Scrapy)**
   - RÃ©cupÃ©ration des livres (titre, prix, catÃ©gorie, disponibilitÃ©, rating, etc.)
   - Export CSV (`out_detail.csv`).

2. **Nettoyage**
   - Conversion des prix (`Â£` â†’ float).
   - Correction des catÃ©gories et des URLs.
   - Uniformisation de la disponibilitÃ©.

3. **Stockage (SQLite)**
   - DonnÃ©es insÃ©rÃ©es dans `books.db` (table `books`).
   - RequÃªtes SQL pour analyser les catÃ©gories, prix moyens, etc.

4. **API REST (FastAPI)**
   - Endpoints :
     - `GET /books?limit=10&sort=title_asc` â†’ liste des livres
     - `GET /books/{id}` â†’ dÃ©tail dâ€™un livre
     - `GET /stats` â†’ stats par catÃ©gorie
   - Lancement avec :
     ```bash
     uvicorn scraping.app:app --reload
     ```

---

## ðŸ›  Installation
```bash
git clone https://github.com/TonPseudo/books_pipeline.git
cd books_pipeline
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## ðŸ“Š Utilisation

1. **Scrapper les donnÃ©es**

```bash
cd scraping
scrapy crawl books -O out_detail.csv
python load_to_sqlite.py
python clean_after_load.py
```

2. **Lancer l'APi**

```bash
uvicorn scraping.app:app --reload
```

3. Tester

  â€¢	http://127.0.0.1:8000/books
	â€¢	http://127.0.0.1:8000/stats

---

## ðŸ“‚ Arborescence

books_pipeline/
â”‚â”€â”€ scraping/
â”‚   â”œâ”€â”€ books_scraper/       # projet Scrapy
â”‚   â”œâ”€â”€ app.py               # FastAPI
â”‚   â”œâ”€â”€ load_to_sqlite.py    # import CSV â†’ SQLite
â”‚   â”œâ”€â”€ clean_after_load.py  # nettoyage donnÃ©es
â”‚   â”œâ”€â”€ books.db             # base SQLite
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ .gitignore

